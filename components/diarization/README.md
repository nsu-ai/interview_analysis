# Diarization

Этот модуль разделяет исходную аудиозапись с несколькими говорящими на отдельные (для каждого из дикторов), а затем распознает в них речевые сегменты, определяя начало и конец каждого сегмента с точностью до миллисекунд. Таким образом решается задача диаризации - определения "кто и когда говорил" во входном аудиопотоке.

В качестве модели speaker separation используется педобученная модель SepFormer от SpeechBrain (<https://huggingface.co/speechbrain/sepformer-whamr-enhancement>). В качестве модели для voice activity detection используется предобученная модель для русского языка Vosk AlphaCephei в своей мобильной версии (<https://alphacephei.com/vosk/models>). Данная архитектура позволяет эффективно определять наложения речи дикторов, так называемые "перебивания".


## Структура

    .
    ├── data                        # Данные (примеры, необходимые маленькие файлы, инструкции по датасетам)
    │   │
    │   └── output_example.json     # Пример выходных данных
    │
    ├── scripts                     # Необходимые для работы модуля скрипты 
    │   │ 
    │   └── vosk_speech_detection.py # Скрипт для voice activity detection с помощью Vosk
    │   │ 
    │   └── diarization_vosk.py     # Скрипт для работы с моделью сепарации аудио, постпроцессинга результатов SepFormer, VAD и финального результата
    │   │ 
    │   └── diarization_service.py  #  Flask-app, предоставляющий сервис для вызова моделей по API. Главное приложение-контроллер модуля.
    ├── models                      # Реализации конкретных моделей
    │   │ 
    │   └── ru_vosk_model           # Папка с файлами модели Vosk для русского языка
    │
    ├── Dockerfile                  # Dockerfile для запуска модуля как сервиса
    ├─- test_diarization.py         # Тестовый файл с примером запроса для модуля
    ├── test1.wav                   # Тестовая аудиозапись разговора двух человек. Для файла с запросом
    ├── test2.wav                   # Тестовая аудиозапись разговора двух человек. Для файла с запросом
    ├── requirements.txt            # Локальные зависимости модуля
    └── README.md (вы здесь)        # Описание модуля

## Сборка и запуск сервиса

Предобученные модели нужно скачать по ссылке <https://alphacephei.com/vosk/models/vosk-model-small-ru-0.22.zip> , разархивировать, переименовать в "ru_vosk_model" и расположить следующим образом:
  * ./models/ru_vosk_model/

Сборка докер-образа с сервисом производится командой
```commandline
sudo docker build -t diarization_service:0.1 .
```
Контейнер запускается командой
```commandline
sudo docker run -p 127.0.0.1:8801:8801 diarization_service:0.1
```

## Вызов сервиса

Для использования модуля как отдельного сервиса нужно отправлять
json файл на endpoint "/recognize" с помощью POST-запроса. Например, 
это можно сделать с помощью библиотеки requests и скрипта на python:

```python
import sys
import requests
import base64
import struct

# Это файл-пример теста модуля диаризации.
# Здесь полученные в результате сепарации аудио сохраняются в текущую директорию. И только для двух дикторов.
# Чтобы сохранять для N дикторов, необходимо небольшое изменение - добавление цикла.

def main():
    requests.get('http://localhost:8801/ready')  # проверяем готовность системы
    if len(sys.argv) > 1:
        sound_name = sys.argv[1]
    else:
        sound_name = 'test1.wav'
    files = {'wav': (sound_name, open(sound_name, 'rb'), 'audio/wave')}
    resp = requests.post('http://localhost:8801/recognize', files=files)
    result_json = resp.json()
    print(result_json["diarization"])
    bytes_json_zero = result_json["dictor_0_audio"].encode("ascii")
    encoded_zero = base64.b64decode(bytes_json_zero)
    with open("dictor_zero.wav", "wb") as outf:
        outf.write(encoded_zero)
    bytes_json_first = result_json["dictor_1_audio"].encode("ascii")
    encoded_first = base64.b64decode(bytes_json_first)
    with open("dictor_first.wav", "wb") as outz:
        outz.write(encoded_first)


if __name__ == '__main__':
    main()
```

#### Входные данные: аудиофайл в формате WAV (wave)

Желательно, чтобы аудиозапись была в формате моно (1 канал), с частотой дискретизации 8000 Гц, но также разработанный модуль имеет метод конвертации входного аудиофайла в необходимый формат.
Файл должен иметь расширение ".wav"


#### Выходные данные: объект json со следующими полями-ключами:

1. diarization - содержит список всех сегментов, в которых была распознана речь. Каждый из сегментов содержит следующие поля:

- speaker - метка диктора, говорящего в конкретный момент аудиозаписи

- start_time - временная отметка начала речи диктора в момент в аудиозаписи с точностью до миллисекунд

- end_time - временная отметка окончания речи диктора в момент в аудиозаписи с точностью до миллисекунд

2. dictor_```<speaker>```_audio - содержит закодированный аудиофайл с дорожкой диктора, артефакт разделения исходного аудио на дорожки, соответсвующие дикторам. В середине ключа-названия каждого файла обозначен номер диктора. Аудио необходимо декодировать из массива байт сначала закодировав его в строку с кодировкой 'ascii', а затем - из байт-строки декодировать в аудиосигнал и записать его в файл. Затем файл необходимо сохранить. Пример работы с аудиозаписями можно найти в тестовом файле-примере test_diarization.py

Пример выходных данных:
```json
{'diarization': [
   {'end_time': '1.763', 'speaker': '1', 'start_time': '0.008'}, 
   {'end_time': '3.350', 'speaker': '0', 'start_time': '2.118'}, 
   {'end_time': '3.147', 'speaker': '1', 'start_time': '2.962'}, 
   {'end_time': '4.582', 'speaker': '0', 'start_time': '3.721'}
   ]}
```

#### Результаты тестирования

Для тестирования пайплайна диаризации был выбран открытый датасет VoxConverse (<https://github.com/joonson/voxconverse>), размеченный для диаризации. 

|  Тестовые данные  | Diarization Error Rate | Всего файлов |
|-------------------|------------------------|--------------|
| VoxConverse (test)|        32,59 %         |      232     |
